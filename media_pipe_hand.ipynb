{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For static images:\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5)\n",
    "\n",
    "names = [\"p\", \"s\", \"r\"]\n",
    "file_list = []\n",
    "result = []\n",
    "\n",
    "for i in names:\n",
    "    for j in range(1,31):\n",
    "        file_list.append(i + str(j) + \".png\")\n",
    "        \n",
    "print(len(file_list))\n",
    "\n",
    "for idx, file in enumerate(file_list):\n",
    "    # Read an image, flip it around y-axis for correct handedness output (see\n",
    "    # above).\n",
    "    image = cv2.flip(cv2.imread(file), 1)\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if not results.multi_hand_landmarks:\n",
    "        continue\n",
    "\n",
    "\n",
    "    image_hight, image_width, _ = image.shape\n",
    "    annotated_image = image.copy()\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        for item in hand_landmarks.landmark:\n",
    "            output.append(item.x)\n",
    "            output.append(item.y)\n",
    "            output.append(item.z)\n",
    "            \n",
    "    result.append(output)\n",
    "\n",
    "hands.close()\n",
    "\n",
    "\n",
    "answer = []\n",
    "\n",
    "for i in range(1,4):\n",
    "    for j in range(30):\n",
    "        answer.append(i)\n",
    "        \n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.transpose(result)\n",
    "y_train = answer\n",
    "\n",
    "inverse = np.linalg.inv(np.matmul(X_train,np.transpose(X_train)))\n",
    "rest = np.matmul(y_train,np.transpose(X_train))\n",
    "W = np.matmul(rest,inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For static images:\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5)\n",
    "\n",
    "\n",
    "\n",
    "test = \"s6.png\"\n",
    "image = cv2.flip(cv2.imread(test), 1)\n",
    "# Convert the BGR image to RGB before processing.\n",
    "results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "image_hight, image_width, _ = image.shape\n",
    "annotated_image = image.copy()\n",
    "\n",
    "output2 = []\n",
    "\n",
    "for hand_landmarks in results.multi_hand_landmarks:\n",
    "    for item in hand_landmarks.landmark:\n",
    "        output2.append(item.x)\n",
    "        output2.append(item.y)\n",
    "        output2.append(item.z)\n",
    "\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = np.transpose(output2)\n",
    "res1 = np.round(np.matmul(np.transpose(W),X_train))\n",
    "\n",
    "print(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "hands = mp_hands.Hands(\n",
    "    min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "  success, image = cap.read()\n",
    "  if not success:\n",
    "    print(\"Ignoring empty camera frame.\")\n",
    "    # If loading a video, use 'break' instead of 'continue'.\n",
    "    continue\n",
    "\n",
    "  # Flip the image horizontally for a later selfie-view display, and convert\n",
    "  # the BGR image to RGB.\n",
    "  image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "  # To improve performance, optionally mark the image as not writeable to\n",
    "  # pass by reference.\n",
    "  image.flags.writeable = False\n",
    "  results = hands.process(image)\n",
    "\n",
    "  result3 = []\n",
    "  # Draw the hand annotations on the image.\n",
    "  image.flags.writeable = True\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "  if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        output3 = []\n",
    "        mp_drawing.draw_landmarks(\n",
    "        image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        for item in hand_landmarks.landmark:\n",
    "            output3.append(item.x)\n",
    "            output3.append(item.y)\n",
    "            output3.append(item.z)\n",
    "        \n",
    "        output_test2 = np.transpose(output3)\n",
    "        res2 = np.round(np.matmul(np.transpose(W),output_test2))\n",
    "\n",
    "        print(res2)\n",
    "    \n",
    "  cv2.imshow('MediaPipe Hands', image)\n",
    "  if cv2.waitKey(5) & 0xFF == 27:\n",
    "    break\n",
    "hands.close()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
